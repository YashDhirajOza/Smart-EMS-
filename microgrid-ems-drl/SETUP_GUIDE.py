"""
COMPLETE SETUP GUIDE
Step-by-step instructions to get your RL microgrid system running
"""

# ========================================
# STEP 1: VERIFY INSTALLATION
# ========================================

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Microgrid EMS with RL - Complete Setup Guide                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

STEP 1: Install Dependencies
----------------------------
Run this command in PowerShell:

    pip install numpy pandas matplotlib seaborn torch gym scipy

Or use the requirements file:

    pip install -r requirements.txt

""")

# ========================================
# STEP 2: TEST COMPONENTS
# ========================================

print("""
STEP 2: Test Components
-----------------------
Verify all modules are working:

    python test_components.py

This will test:
- Configuration loading
- Battery degradation model
- EV fleet simulator
- Safety supervisor  
- Data preprocessing
- Gym environment
- PPO agent

""")

# ========================================
# STEP 3: PREPARE DATA
# ========================================

print("""
STEP 3: Prepare Data
--------------------
You have two options:

OPTION A - Use Your Solar Plant Data:
    python data_preprocessing.py

This will:
- Load Plant_1_Generation_Data.csv
- Load Plant_1_Weather_Sensor_Data.csv  
- Process to 15-minute intervals
- Generate wind, load, price profiles
- Save to data/ folder

OPTION B - Use Synthetic Data:
The test script already creates synthetic data if real data is missing.
You can skip to Step 4.

Expected output files in data/:
- pv_profile_processed.csv
- wt_profile_processed.csv
- load_profile_processed.csv
- price_profile_processed.csv

""")

# ========================================
# STEP 4: TRAIN THE AGENT
# ========================================

print("""
STEP 4: Train the RL Agent
---------------------------
Start training with PPO:

    python train_ppo.py

Training settings (configurable in env_config.py):
- Episodes: 500
- Algorithm: PPO
- Learning rate: 3e-4
- Batch size: 64
- Save interval: 100 episodes
- Log interval: 10 episodes

Training will take several hours depending on your hardware.

Outputs:
- models/ppo_TIMESTAMP/best_model.pt
- models/ppo_TIMESTAMP/checkpoint_epXXX.pt
- logs/ppo_TIMESTAMP/training_metrics.csv

Monitor training progress:
- Watch console output every 10 episodes
- Check logs/training_metrics.csv for detailed curves

""")

# ========================================
# STEP 5: EVALUATE THE AGENT
# ========================================

print("""
STEP 5: Evaluate the Trained Agent
-----------------------------------
Compare your RL agent against baselines:

    python evaluate.py

This will:
- Load best_model.pt
- Run 20 test episodes
- Compare vs Rule-Based TOU, Greedy, Random
- Generate plots and metrics

Outputs:
- evaluation/controller_comparison.csv
- evaluation/comparison_bars.png
- evaluation/rl_trajectory.png
- evaluation/rl_detailed_metrics.csv

Metrics reported:
- Total cost ($)
- Total emissions (kg COâ‚‚)
- Unmet demand events (should be 0)
- Safety violations
- Battery degradation
- EV charging success rate

""")

# ========================================
# STEP 6: CUSTOMIZE & EXPERIMENT
# ========================================

print("""
STEP 6: Customize & Experiment
-------------------------------
Modify env_config.py to experiment:

Key parameters to tune:

1. Reward weights (REWARD config):
   - alpha: Emission cost weight (0.01-0.1)
   - beta: Degradation weight (0.1-1.0)
   - gamma: Reliability penalty (1000+)

2. Battery configuration (BATTERY_5, BATTERY_10):
   - capacity_kwh: Energy capacity
   - max_charge_kw / max_discharge_kw: Power limits
   - soc_min / soc_max: Operating range
   - cycle_life: Expected lifetime cycles

3. EV fleet (EV_FLEET):
   - max_concurrent_evs: Fleet size
   - ev_battery_capacity_range: EV battery sizes
   - ev_arrival_soc_range: Arrival SoC distribution

4. Training (TRAINING):
   - learning_rate_actor / learning_rate_critic
   - ppo_batch_size: Batch size
   - ppo_epochs: Update epochs
   - forecast_noise_std: Forecast uncertainty

5. Safety (SAFETY):
   - soc_violation_threshold: SoC tolerance
   - safety_override_penalty: Penalty for violations

After making changes:
    python train_ppo.py --config my_config.py

""")

# ========================================
# TROUBLESHOOTING
# ========================================

print("""
TROUBLESHOOTING
---------------

Problem: "FileNotFoundError: Plant_1_Generation_Data.csv"
Solution: Either:
  1. Place your CSV files in the project root, OR
  2. Let test_components.py create synthetic data

Problem: "Training is unstable / reward not improving"
Solution:
  - Reduce learning rate (try 1e-4)
  - Increase batch size (try 128)
  - Check reward weights (gamma should be very large)
  - Enable observation normalization

Problem: "Agent violates SoC constraints frequently"
Solution:
  - Increase safety_override_penalty in SAFETY config
  - Check battery SoC limits (soc_min, soc_max)
  - Verify battery capacity vs power ratings

Problem: "High unmet demand events"
Solution:
  - Increase gamma (reliability penalty) to 5000+
  - Check grid import limit vs load demand
  - Verify renewable + battery capacity is sufficient

Problem: "Agent ignores EVs"
Solution:
  - Increase EV-related features in reward
  - Use curriculum learning (start without EVs, add gradually)
  - Check EV arrival patterns are reasonable

Problem: "Out of memory during training"
Solution:
  - Reduce batch size
  - Reduce buffer size (for SAC)
  - Use smaller network architectures
  - Train on fewer episodes per update

Problem: "Training too slow"
Solution:
  - Use GPU if available (torch.cuda)
  - Reduce number of episodes
  - Simplify environment (disable degradation/EVs initially)
  - Use vectorized environments

""")

# ========================================
# ADVANCED USAGE
# ========================================

print("""
ADVANCED USAGE
--------------

1. Implement SAC (Soft Actor-Critic):
   - Create train_sac.py based on train_ppo.py
   - SAC typically more sample-efficient than PPO
   - Better for continuous action spaces

2. Curriculum Learning:
   Progressively increase difficulty:
   - Stage 1: No EVs, perfect forecasts (500 ep)
   - Stage 2: Add forecast noise (500 ep)
   - Stage 3: Add EVs, simple (1000 ep)
   - Stage 4: Full complexity (2000 ep)

3. Real-time Deployment:
   - Export trained model
   - Create inference wrapper
   - Add MPC backup controller
   - Log all decisions for safety

4. Multi-objective Pareto Optimization:
   - Train multiple agents with different reward weights
   - Plot Pareto frontier (cost vs emissions)
   - Let operator choose preferred trade-off

5. Transfer Learning:
   - Pre-train on synthetic data
   - Fine-tune on real data
   - Reduces training time significantly

6. Model-Based RL:
   - Learn dynamics model of microgrid
   - Use model for planning
   - Combine with model-free RL

""")

# ========================================
# EXPECTED RESULTS
# ========================================

print("""
EXPECTED RESULTS
----------------

After successful training, you should see:

vs. Rule-Based TOU Controller:
- Cost reduction: 15-30%
- Emission reduction: 10-20%
- Better battery utilization (lower DoD)
- Zero unmet demand (same as baseline)

vs. Greedy Controller:
- Cost reduction: 20-40%
- Emission reduction: 15-25%
- Better long-term planning
- Reduced battery degradation

Training Curves:
- Episode return should increase over time
- Cost should decrease over time
- Unmet demand events should drop to zero
- Safety violations should decrease

Typical Final Performance (after 500 episodes):
- Average episode return: -500 to -200
- Average cost: $500-800 per 24h episode
- Average emissions: 2000-3000 kg COâ‚‚ per episode
- Unmet demand events: 0
- Safety overrides: <5 per episode
- EV charging success rate: >90%

""")

# ========================================
# NEXT STEPS
# ========================================

print("""
NEXT STEPS & RESEARCH DIRECTIONS
---------------------------------

1. Improve Degradation Model:
   - Implement electrochemical models
   - Add temperature-dependent aging
   - Validate with battery manufacturer data

2. V2G (Vehicle-to-Grid):
   - Allow bidirectional EV charging
   - EVs can supply grid during peak
   - More complex optimization

3. Demand Response:
   - Integrate demand response programs
   - Shift flexible loads
   - Optimize for multiple time horizons

4. Multi-Microgrid Coordination:
   - Multiple interconnected microgrids
   - Multi-agent RL
   - Energy trading between microgrids

5. Uncertainty Quantification:
   - Probabilistic forecasts
   - Risk-aware optimization
   - Robust control strategies

6. Hardware-in-the-Loop:
   - Connect to real battery/inverter
   - Test in laboratory environment
   - Validate before full deployment

7. Economic Analysis:
   - Detailed cost-benefit analysis
   - ROI calculations
   - Compare vs traditional control

8. Regulatory Compliance:
   - Grid code requirements
   - Frequency regulation
   - Voltage support

""")

# ========================================
# SUMMARY
# ========================================

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  QUICK REFERENCE                                               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Test everything:      python test_components.py
Process data:         python data_preprocessing.py
Train PPO agent:      python train_ppo.py
Evaluate agent:       python evaluate.py

Key files to edit:
- env_config.py:      All hyperparameters & configuration
- train_ppo.py:       Training loop & PPO implementation
- microgrid_env.py:   Environment dynamics
- evaluate.py:        Evaluation & comparison

Key directories:
- data/:             Processed data profiles
- models/:           Trained model checkpoints
- logs/:             Training metrics & curves
- evaluation/:       Evaluation results & plots

Documentation:
- README.md:         Project overview & features
- SETUP_GUIDE.py:    This file
- Comments in code:  Detailed explanations

Support:
- Open GitHub issue for bugs
- Check logs for error messages
- Read error traces carefully

Happy training! ğŸš€âš¡ğŸ”‹

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  Your RL agent will learn to:                                 â•‘
â•‘  âœ“ Minimize costs while reducing emissions                    â•‘
â•‘  âœ“ Optimize battery usage (reduce degradation)                â•‘
â•‘  âœ“ Coordinate EV charging with grid constraints               â•‘
â•‘  âœ“ Never leave demand unmet (reliability guarantee)           â•‘
â•‘  âœ“ Explain its decisions to operators                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")
